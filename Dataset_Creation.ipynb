{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxxFjHc9+tNl35cWOEO4Ma",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Buddika-Kasun/ML/blob/main/Dataset_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jN0p5Nge9rFC"
      },
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "!pip install mediapipe opencv-python pandas numpy tqdm\n",
        "!apt update && apt install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import shutil"
      ],
      "metadata": {
        "id": "KhBW2heG-KKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (most reliable for large files)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def copy_from_drive():\n",
        "    \"\"\"Copy your structured folder from Google Drive\"\"\"\n",
        "    # Your folder structure in Google Drive\n",
        "    drive_path = '/content/drive/MyDrive/SLSL_Dataset'  # Adjust this path\n",
        "\n",
        "    if os.path.exists(drive_path):\n",
        "        # Copy entire structure\n",
        "        import shutil\n",
        "        shutil.copytree(drive_path, '/content/raw_videos', dirs_exist_ok=True)\n",
        "        print(\"‚úÖ Copied entire structure from Google Drive!\")\n",
        "\n",
        "        # Show what was copied\n",
        "        sentences = os.listdir('/content/raw_videos')\n",
        "        print(f\"üìÅ Sentence folders copied: {len(sentences)}\")\n",
        "        for sentence in sentences:\n",
        "            sentence_path = f\"/content/raw_videos/{sentence}\"\n",
        "            video_count = len([f for f in os.listdir(sentence_path) if f.endswith('.mp4')])\n",
        "            print(f\"   {sentence}: {video_count} videos\")\n",
        "    else:\n",
        "        print(\"‚ùå SLSL_Dataset folder not found in Google Drive\")\n",
        "        print(\"üí° Please make sure your folder is at: /content/drive/MyDrive/SLSL_Dataset\")\n",
        "\n",
        "# Mount and copy\n",
        "drive.mount('/content/drive')\n",
        "copy_from_drive()"
      ],
      "metadata": {
        "id": "DoGytTfdeE_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MediaPipe Holistic\n",
        "mp_holistic = mp.solutions.holistic\n",
        "mp_drawing = mp.solutions.drawing_utils"
      ],
      "metadata": {
        "id": "2r0VWJ9_-F3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_mediapipe():\n",
        "    \"\"\"Setup MediaPipe Holistic with optimal settings for SLSL\"\"\"\n",
        "    holistic = mp_holistic.Holistic(\n",
        "        static_image_mode=False,        # False for videos\n",
        "        model_complexity=1,             # 1 for balanced accuracy/speed\n",
        "        smooth_landmarks=True,          # Temporal smoothing\n",
        "        enable_segmentation=False,      # Not needed for landmarks\n",
        "        smooth_segmentation=True,\n",
        "        refine_face_landmarks=True,     # CRITICAL for accurate lip landmarks\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    )\n",
        "    return holistic"
      ],
      "metadata": {
        "id": "tpGlIi6R-0As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "holistic = setup_mediapipe()\n",
        "print(\"MediaPipe Holistic initialized!\")"
      ],
      "metadata": {
        "id": "JgGWhUEcYXAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frame_landmarks(frame, holistic_model):\n",
        "    \"\"\"\n",
        "    Extract hand, pose, and lip landmarks from a single frame\n",
        "    Returns dictionary with all landmarks\n",
        "    \"\"\"\n",
        "    # Convert BGR to RGB\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame_rgb.flags.writeable = False\n",
        "\n",
        "    # Process with MediaPipe\n",
        "    results = holistic_model.process(frame_rgb)\n",
        "\n",
        "    landmarks_dict = {\n",
        "        'left_hand': None,\n",
        "        'right_hand': None,\n",
        "        'pose': None,\n",
        "        'face': None,\n",
        "        'lip_roi': None,\n",
        "        'timestamp': None\n",
        "    }\n",
        "\n",
        "    # Extract Left Hand Landmarks (21 points, 3 coordinates each = 63 values)\n",
        "    if results.left_hand_landmarks:\n",
        "        left_hand = []\n",
        "        for landmark in results.left_hand_landmarks.landmark:\n",
        "            left_hand.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "        landmarks_dict['left_hand'] = left_hand\n",
        "\n",
        "    # Extract Right Hand Landmarks\n",
        "    if results.right_hand_landmarks:\n",
        "        right_hand = []\n",
        "        for landmark in results.right_hand_landmarks.landmark:\n",
        "            right_hand.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "        landmarks_dict['right_hand'] = right_hand\n",
        "\n",
        "    # Extract Pose Landmarks (Upper body - 25 points)\n",
        "    if results.pose_landmarks:\n",
        "        pose = []\n",
        "        upper_body_indices = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]  # Upper body\n",
        "        for i in upper_body_indices:\n",
        "            if i < len(results.pose_landmarks.landmark):\n",
        "                landmark = results.pose_landmarks.landmark[i]\n",
        "                pose.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "        landmarks_dict['pose'] = pose\n",
        "\n",
        "    # Extract Lip Landmarks\n",
        "    if results.face_landmarks:\n",
        "        # Lip landmark indices from MediaPipe Face Mesh (simplified set)\n",
        "        lip_indices = [\n",
        "            # Outer lips\n",
        "            61, 84, 314, 17, 87, 178, 88, 95, 78, 62, 96, 89,\n",
        "            146, 91, 181, 76, 184, 74, 183, 42,\n",
        "            # Inner lips\n",
        "            13, 82, 81, 80, 191, 78, 312, 311, 310, 415, 308, 324,\n",
        "            318, 402, 317, 14, 87, 178\n",
        "        ]\n",
        "\n",
        "        lip_landmarks = []\n",
        "        face_landmarks = []\n",
        "\n",
        "        for i, landmark in enumerate(results.face_landmarks.landmark):\n",
        "            face_landmarks.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "            if i in lip_indices:\n",
        "                lip_landmarks.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "\n",
        "        landmarks_dict['face'] = face_landmarks\n",
        "        landmarks_dict['lip_roi'] = lip_landmarks\n",
        "\n",
        "    return landmarks_dict"
      ],
      "metadata": {
        "id": "uickc9XuYSBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_video(video_path, holistic_model, max_frames=None):\n",
        "    \"\"\"\n",
        "    Process a single video and extract landmarks from all frames\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames_data = []\n",
        "    frame_count = 0\n",
        "\n",
        "    # Get video properties\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"üìπ Processing: {os.path.basename(video_path)}\")\n",
        "    print(f\"   Frames: {total_frames}, FPS: {fps}\")\n",
        "\n",
        "    with tqdm(total=total_frames, desc=\"Extracting landmarks\") as pbar:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if max_frames and frame_count >= max_frames:\n",
        "                break\n",
        "\n",
        "            # Extract landmarks\n",
        "            landmarks = extract_frame_landmarks(frame, holistic_model)\n",
        "            landmarks['timestamp'] = frame_count / fps  # Add timestamp\n",
        "            frames_data.append(landmarks)\n",
        "\n",
        "            frame_count += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"   ‚úÖ Extracted {len(frames_data)} frames\")\n",
        "    return frames_data"
      ],
      "metadata": {
        "id": "tCrPo3UGY_sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sentence_based_dataset(raw_videos_root='/content/raw_videos',\n",
        "                                 output_folder='/content/landmarks_data',\n",
        "                                 test_mode=False):\n",
        "    \"\"\"\n",
        "    Main function to process the entire sentence-based dataset\n",
        "    \"\"\"\n",
        "    metadata = []\n",
        "\n",
        "    print(\"üîç Scanning for sentence folders...\")\n",
        "\n",
        "    # Find all sentence folders\n",
        "    sentence_folders = []\n",
        "    for item in os.listdir(raw_videos_root):\n",
        "        item_path = os.path.join(raw_videos_root, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            sentence_folders.append(item)\n",
        "\n",
        "    print(f\"üìÅ Found {len(sentence_folders)} sentence folders: {sentence_folders}\")\n",
        "\n",
        "    # Process each sentence folder\n",
        "    for sentence in sentence_folders:\n",
        "        sentence_path = os.path.join(raw_videos_root, sentence)\n",
        "        video_files = [f for f in os.listdir(sentence_path)\n",
        "                      if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
        "\n",
        "        print(f\"\\nüé¨ Processing sentence: '{sentence}'\")\n",
        "        print(f\"   Found {len(video_files)} videos\")\n",
        "\n",
        "        # Test mode: process only 1 video per sentence\n",
        "        if test_mode:\n",
        "            video_files = video_files[:1]\n",
        "            print(f\"   TEST MODE: Processing only 1 video\")\n",
        "\n",
        "        for video_file in video_files:\n",
        "            video_path = os.path.join(sentence_path, video_file)\n",
        "\n",
        "            # Parse filename (expected: signer_01_rep_1.mp4)\n",
        "            filename_parts = video_file.replace('.mp4', '').split('_')\n",
        "            signer_id = filename_parts[1] if len(filename_parts) > 1 else \"unknown\"\n",
        "            rep_number = filename_parts[3] if len(filename_parts) > 3 else \"1\"\n",
        "\n",
        "            try:\n",
        "                # Process video (limit frames in test mode)\n",
        "                max_frames = 50 if test_mode else None\n",
        "                landmarks_sequence = process_single_video(video_path, holistic, max_frames)\n",
        "\n",
        "                if landmarks_sequence:\n",
        "                    # Create output filename\n",
        "                    output_filename = f\"{sentence}_signer_{signer_id}_rep_{rep_number}.npy\"\n",
        "                    output_path = os.path.join(output_folder, output_filename)\n",
        "\n",
        "                    # Save landmarks\n",
        "                    np.save(output_path, landmarks_sequence)\n",
        "\n",
        "                    # Calculate landmark statistics\n",
        "                    left_hand_frames = sum(1 for frame in landmarks_sequence if frame['left_hand'] is not None)\n",
        "                    right_hand_frames = sum(1 for frame in landmarks_sequence if frame['right_hand'] is not None)\n",
        "                    lip_frames = sum(1 for frame in landmarks_sequence if frame['lip_roi'] is not None)\n",
        "\n",
        "                    # Add to metadata\n",
        "                    metadata.append({\n",
        "                        'landmarks_file': output_filename,\n",
        "                        'sentence': sentence,\n",
        "                        'signer_id': signer_id,\n",
        "                        'rep_number': rep_number,\n",
        "                        'original_video': video_file,\n",
        "                        'video_path': video_path,\n",
        "                        'total_frames': len(landmarks_sequence),\n",
        "                        'left_hand_frames': left_hand_frames,\n",
        "                        'right_hand_frames': right_hand_frames,\n",
        "                        'lip_frames': lip_frames,\n",
        "                        'left_hand_coverage': (left_hand_frames / len(landmarks_sequence)) * 100,\n",
        "                        'right_hand_coverage': (right_hand_frames / len(landmarks_sequence)) * 100,\n",
        "                        'lip_coverage': (lip_frames / len(landmarks_sequence)) * 100,\n",
        "                        'success': True\n",
        "                    })\n",
        "\n",
        "                    print(f\"   ‚úÖ Saved: {output_filename}\")\n",
        "                else:\n",
        "                    print(f\"   ‚ùå No landmarks extracted: {video_file}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error processing {video_file}: {str(e)}\")\n",
        "                metadata.append({\n",
        "                    'landmarks_file': 'FAILED',\n",
        "                    'sentence': sentence,\n",
        "                    'signer_id': signer_id,\n",
        "                    'rep_number': rep_number,\n",
        "                    'original_video': video_file,\n",
        "                    'error': str(e),\n",
        "                    'success': False\n",
        "                })\n",
        "\n",
        "    # Save metadata\n",
        "    if metadata:\n",
        "        metadata_df = pd.DataFrame(metadata)\n",
        "        metadata_path = '/content/metadata/sentence_dataset_metadata.csv'\n",
        "        metadata_df.to_csv(metadata_path, index=False)\n",
        "\n",
        "        # Save sentence mapping\n",
        "        unique_sentences = metadata_df['sentence'].unique()\n",
        "        sentence_mapping = {sentence: idx for idx, sentence in enumerate(unique_sentences)}\n",
        "\n",
        "        with open('/content/metadata/sentence_mapping.json', 'w') as f:\n",
        "            json.dump(sentence_mapping, f, indent=2)\n",
        "\n",
        "        print(f\"\\nüìä Metadata saved: {metadata_path}\")\n",
        "        print(f\"üìù Sentence mapping saved: /content/metadata/sentence_mapping.json\")\n",
        "\n",
        "        return metadata_df, sentence_mapping\n",
        "    else:\n",
        "        print(\"‚ùå No videos processed successfully!\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "bnNt7KieZRqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentence_dataset(metadata_df):\n",
        "    \"\"\"Comprehensive analysis of the sentence-based dataset\"\"\"\n",
        "    print(\"üìä SENTENCE DATASET ANALYSIS REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Basic statistics\n",
        "    total_videos = len(metadata_df)\n",
        "    successful_videos = metadata_df['success'].sum()\n",
        "\n",
        "    print(f\"\\nüìà BASIC STATISTICS:\")\n",
        "    print(f\"   Total videos processed: {total_videos}\")\n",
        "    print(f\"   Successful processing: {successful_videos}\")\n",
        "    print(f\"   Success rate: {(successful_videos/total_videos)*100:.1f}%\")\n",
        "\n",
        "    # By sentence\n",
        "    print(f\"\\nüìù BY SENTENCE:\")\n",
        "    sentence_stats = metadata_df[metadata_df['success']]['sentence'].value_counts()\n",
        "    for sentence, count in sentence_stats.items():\n",
        "        print(f\"   '{sentence}': {count} videos\")\n",
        "\n",
        "    # By signer\n",
        "    print(f\"\\nüë• BY SIGNER:\")\n",
        "    signer_stats = metadata_df[metadata_df['success']]['signer_id'].value_counts()\n",
        "    for signer, count in signer_stats.items():\n",
        "        print(f\"   Signer {signer}: {count} videos\")\n",
        "\n",
        "    # Landmark coverage\n",
        "    if successful_videos > 0:\n",
        "        successful_df = metadata_df[metadata_df['success']]\n",
        "        avg_left_hand = successful_df['left_hand_coverage'].mean()\n",
        "        avg_right_hand = successful_df['right_hand_coverage'].mean()\n",
        "        avg_lips = successful_df['lip_coverage'].mean()\n",
        "\n",
        "        print(f\"\\nüñêÔ∏è AVERAGE LANDMARK COVERAGE:\")\n",
        "        print(f\"   Left Hand: {avg_left_hand:.1f}%\")\n",
        "        print(f\"   Right Hand: {avg_right_hand:.1f}%\")\n",
        "        print(f\"   Lip Landmarks: {avg_lips:.1f}%\")\n",
        "\n",
        "    # Frame statistics\n",
        "    if successful_videos > 0:\n",
        "        avg_frames = successful_df['total_frames'].mean()\n",
        "        total_frames = successful_df['total_frames'].sum()\n",
        "        print(f\"\\nüéûÔ∏è FRAME STATISTICS:\")\n",
        "        print(f\"   Average frames per video: {avg_frames:.0f}\")\n",
        "        print(f\"   Total frames processed: {total_frames}\")\n",
        "\n",
        "def verify_landmark_files(landmarks_folder='/content/landmarks_data'):\n",
        "    \"\"\"Verify the created landmark files\"\"\"\n",
        "    print(\"\\nüîç VERIFYING LANDMARK FILES...\")\n",
        "\n",
        "    landmark_files = [f for f in os.listdir(landmarks_folder) if f.endswith('.npy')]\n",
        "    print(f\"   Found {len(landmark_files)} landmark files\")\n",
        "\n",
        "    if landmark_files:\n",
        "        # Check first 3 files\n",
        "        for i, file in enumerate(landmark_files[:3]):\n",
        "            file_path = os.path.join(landmarks_folder, file)\n",
        "            data = np.load(file_path, allow_pickle=True)\n",
        "\n",
        "            print(f\"\\n   Sample {i+1}: {file}\")\n",
        "            print(f\"     Frames: {len(data)}\")\n",
        "            if len(data) > 0:\n",
        "                print(f\"     Keys: {list(data[0].keys())}\")\n",
        "                print(f\"     Left hand present: {data[0]['left_hand'] is not None}\")\n",
        "                print(f\"     Right hand present: {data[0]['right_hand'] is not None}\")\n",
        "                print(f\"     Lip landmarks present: {data[0]['lip_roi'] is not None}\")"
      ],
      "metadata": {
        "id": "Ri17vdSUaB59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline(test_mode=True):\n",
        "    \"\"\"\n",
        "    Complete pipeline for sentence-based dataset creation\n",
        "    Set test_mode=False for full processing\n",
        "    \"\"\"\n",
        "    print(\"üöÄ STARTING SENTENCE-BASED SLSL DATASET CREATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Setup\n",
        "    print(\"\\nüìã STEP 1: Environment Setup\")\n",
        "    create_sentence_based_structure()\n",
        "\n",
        "    # Step 2: Initialize MediaPipe\n",
        "    print(\"\\nüìã STEP 2: MediaPipe Initialization\")\n",
        "    global holistic\n",
        "    holistic = setup_mediapipe()\n",
        "\n",
        "    # Step 3: Process dataset\n",
        "    print(\"\\nüìã STEP 3: Processing Videos\")\n",
        "    print(\"üí° Upload your videos to sentence folders in '/content/raw_videos/'\")\n",
        "    print(\"   Folder structure:\")\n",
        "    print(\"   raw_videos/\")\n",
        "    print(\"   ‚îú‚îÄ‚îÄ where_does_it_hurt/\")\n",
        "    print(\"   ‚îÇ   ‚îú‚îÄ‚îÄ signer_01_rep_1.mp4\")\n",
        "    print(\"   ‚îÇ   ‚îî‚îÄ‚îÄ ...\")\n",
        "    print(\"   ‚îú‚îÄ‚îÄ i_have_a_headache/\")\n",
        "    print(\"   ‚îÇ   ‚îî‚îÄ‚îÄ ...\")\n",
        "    print(\"   ‚îî‚îÄ‚îÄ ...\")\n",
        "\n",
        "    input(\"‚è∞ Press Enter after uploading videos...\")\n",
        "\n",
        "    # Step 4: Process videos\n",
        "    metadata_df, sentence_mapping = process_sentence_based_dataset(test_mode=test_mode)\n",
        "\n",
        "    # Step 5: Analyze results\n",
        "    if metadata_df is not None:\n",
        "        print(\"\\nüìã STEP 4: Dataset Analysis\")\n",
        "        analyze_sentence_dataset(metadata_df)\n",
        "\n",
        "        print(\"\\nüìã STEP 5: Verification\")\n",
        "        verify_landmark_files()\n",
        "\n",
        "        print(f\"\\nüéâ DATASET CREATION COMPLETE!\")\n",
        "        print(f\"üìÅ Landmarks saved in: /content/landmarks_data/\")\n",
        "        print(f\"üìä Metadata saved in: /content/metadata/\")\n",
        "\n",
        "        return metadata_df, sentence_mapping\n",
        "    else:\n",
        "        print(\"‚ùå Dataset creation failed!\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "tSX5SI-taM7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üéØ EXECUTE THE PIPELINE\n",
        "# For testing (process 1 video per sentence)\n",
        "metadata, mapping = main_pipeline(test_mode=True)"
      ],
      "metadata": {
        "id": "pf_LakWna-XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For full processing (uncomment when ready)\n",
        "# metadata, mapping = main_pipeline(test_mode=False)"
      ],
      "metadata": {
        "id": "4xhtCPKObAuu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}